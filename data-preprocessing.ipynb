{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Alchemy: Unveiling Your Next Favorite through Machine Learning\n",
    "\n",
    "Anime, a realm where imagination knows no bounds, enthralls viewers with its diverse worlds and compelling narratives. Yet, the vast expanse of anime choices can leave enthusiasts overwhelmed when selecting their next watch. This is where the fusion of machine learning and anime enters the stage. By harnessing the capabilities of algorithms and data analysis, machine learning offers a transformative solution to the eternal question: \"What anime should I watch next?\"\n",
    "\n",
    "Gone are the days of aimlessly scrolling through recommendations or relying on random selections. Machine learning algorithms, fueled by the wealth of data from fans and streaming platforms, now provide tailored suggestions that resonate with individual preferences. In this exploration of anime and machine learning, we unravel the mechanics behind recommendation systems, the data they rely on, and the techniques that empower them to accurately predict your next anime obsession.\n",
    "\n",
    "From collaborative and content-based filtering to hybrid models and deep learning, we'll demystify these algorithms and their ability to capture the essence of what makes a show captivating. However, beyond the technical underpinnings, we'll also delve into the intricate connection between data-driven insights and the artistry that underpins every anime tale.\n",
    "\n",
    "Whether you're an anime veteran seeking fresh adventures or a newcomer ready to embark on an animated journey, join us as we navigate the world of anime recommendations powered by machine learning. Together, we'll uncover how this fusion of technology and creativity is reshaping the way we indulge in the enchanting realm of anime. - Thanks ChatGPT\n",
    "\n",
    "I will cover the following things, probably in separate pages:\n",
    "- Collaborative Filtering using Single Value Decomposition\n",
    "- Collobarative Filtering using Neural Nets\n",
    "- Hybrid Collobarative Filtering.\n",
    "\n",
    "Data used for this project contains 300k unique users. Thank you Azathoth for saving me from needing to scrape the data myself. (It took me 5-10 minutes per user to scrape all the review data, and I quickly realised that it was going to take too long.) \n",
    "\n",
    "Link to the data is here on kaggle: https://www.kaggle.com/datasets/azathoth42/myanimelist?select=users_cleaned.csv\n",
    "\n",
    "The aim from the data I have is the achieve a clean set of data for user id, anime id which are the features (X) and rating (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_raw = pd.read_csv('./data/full/anime_cleaned.csv')\n",
    "scores_raw = pd.read_csv('./data/full/animelists_cleaned.csv')\n",
    "users_raw = pd.read_csv('./data/full/users_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_watching</th>\n",
       "      <th>user_completed</th>\n",
       "      <th>user_onhold</th>\n",
       "      <th>user_dropped</th>\n",
       "      <th>user_plantowatch</th>\n",
       "      <th>user_days_spent_watching</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>access_rank</th>\n",
       "      <th>join_date</th>\n",
       "      <th>last_online</th>\n",
       "      <th>stats_mean_score</th>\n",
       "      <th>stats_rewatched</th>\n",
       "      <th>stats_episodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>2255153</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.091667</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chennai, India</td>\n",
       "      <td>1990-04-29 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-03-03 00:00:00</td>\n",
       "      <td>2014-02-04 01:32:00</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Damonashu</td>\n",
       "      <td>37326</td>\n",
       "      <td>45</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>59</td>\n",
       "      <td>82.574306</td>\n",
       "      <td>Male</td>\n",
       "      <td>Detroit,Michigan</td>\n",
       "      <td>1991-08-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-02-13 00:00:00</td>\n",
       "      <td>2017-07-10 06:52:54</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bskai</td>\n",
       "      <td>228342</td>\n",
       "      <td>25</td>\n",
       "      <td>414</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>159.483333</td>\n",
       "      <td>Male</td>\n",
       "      <td>Nayarit, Mexico</td>\n",
       "      <td>1990-12-14 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-08-31 00:00:00</td>\n",
       "      <td>2014-05-12 16:35:00</td>\n",
       "      <td>8.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>terune_uzumaki</td>\n",
       "      <td>327311</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.394444</td>\n",
       "      <td>Female</td>\n",
       "      <td>Malaysia, Kuantan</td>\n",
       "      <td>1998-08-24 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-05-10 00:00:00</td>\n",
       "      <td>2012-10-18 19:06:00</td>\n",
       "      <td>9.70</td>\n",
       "      <td>6.0</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bas_G</td>\n",
       "      <td>5015094</td>\n",
       "      <td>35</td>\n",
       "      <td>114</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>175</td>\n",
       "      <td>30.458333</td>\n",
       "      <td>Male</td>\n",
       "      <td>Nijmegen, Nederland</td>\n",
       "      <td>1999-10-24 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-26 00:00:00</td>\n",
       "      <td>2018-05-10 20:53:37</td>\n",
       "      <td>7.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         username  user_id  user_watching  user_completed  user_onhold  \\\n",
       "0        karthiga  2255153              3              49            1   \n",
       "1       Damonashu    37326             45             195           27   \n",
       "2           bskai   228342             25             414            2   \n",
       "3  terune_uzumaki   327311              5               5            0   \n",
       "4           Bas_G  5015094             35             114            6   \n",
       "\n",
       "   user_dropped  user_plantowatch  user_days_spent_watching  gender  \\\n",
       "0             0                 0                 55.091667  Female   \n",
       "1            25                59                 82.574306    Male   \n",
       "2             5                11                159.483333    Male   \n",
       "3             0                 0                 11.394444  Female   \n",
       "4            20               175                 30.458333    Male   \n",
       "\n",
       "              location           birth_date  access_rank            join_date  \\\n",
       "0      Chennai, India   1990-04-29 00:00:00          NaN  2013-03-03 00:00:00   \n",
       "1     Detroit,Michigan  1991-08-01 00:00:00          NaN  2008-02-13 00:00:00   \n",
       "2      Nayarit, Mexico  1990-12-14 00:00:00          NaN  2009-08-31 00:00:00   \n",
       "3    Malaysia, Kuantan  1998-08-24 00:00:00          NaN  2010-05-10 00:00:00   \n",
       "4  Nijmegen, Nederland  1999-10-24 00:00:00          NaN  2015-11-26 00:00:00   \n",
       "\n",
       "           last_online  stats_mean_score  stats_rewatched  stats_episodes  \n",
       "0  2014-02-04 01:32:00              7.43              0.0            3391  \n",
       "1  2017-07-10 06:52:54              6.15              6.0            4903  \n",
       "2  2014-05-12 16:35:00              8.27              1.0            9701  \n",
       "3  2012-10-18 19:06:00              9.70              6.0             697  \n",
       "4  2018-05-10 20:53:37              7.86              0.0            1847  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need user_id and user_name\n",
    "users_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "username, user_id will be useful here.\n",
    "\n",
    "Interestingly \"stats_mean_score\" suggests that each person will have different clusters of ratings. There was some thought to renormalise the ratings for each user based on their mean score. However I decided against it as some users may only be willing to review good or users may only be willing to review bad anime. I cannot distinguish this from optimisistic reviewers and pessimistic reviews respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>my_watched_episodes</th>\n",
       "      <th>my_start_date</th>\n",
       "      <th>my_finish_date</th>\n",
       "      <th>my_score</th>\n",
       "      <th>my_status</th>\n",
       "      <th>my_rewatching</th>\n",
       "      <th>my_rewatching_ep</th>\n",
       "      <th>my_last_updated</th>\n",
       "      <th>my_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>21</td>\n",
       "      <td>586</td>\n",
       "      <td>0000-00-00</td>\n",
       "      <td>0000-00-00</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-03 10:52:53</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>59</td>\n",
       "      <td>26</td>\n",
       "      <td>0000-00-00</td>\n",
       "      <td>0000-00-00</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-10 13:54:51</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>74</td>\n",
       "      <td>26</td>\n",
       "      <td>0000-00-00</td>\n",
       "      <td>0000-00-00</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-27 16:43:35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>120</td>\n",
       "      <td>26</td>\n",
       "      <td>0000-00-00</td>\n",
       "      <td>0000-00-00</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-03 10:53:57</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>178</td>\n",
       "      <td>26</td>\n",
       "      <td>0000-00-00</td>\n",
       "      <td>0000-00-00</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-27 15:59:13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   username  anime_id  my_watched_episodes my_start_date my_finish_date  \\\n",
       "0  karthiga        21                  586    0000-00-00     0000-00-00   \n",
       "1  karthiga        59                   26    0000-00-00     0000-00-00   \n",
       "2  karthiga        74                   26    0000-00-00     0000-00-00   \n",
       "3  karthiga       120                   26    0000-00-00     0000-00-00   \n",
       "4  karthiga       178                   26    0000-00-00     0000-00-00   \n",
       "\n",
       "   my_score  my_status  my_rewatching  my_rewatching_ep      my_last_updated  \\\n",
       "0         9          1            NaN                 0  2013-03-03 10:52:53   \n",
       "1         7          2            NaN                 0  2013-03-10 13:54:51   \n",
       "2         7          2            NaN                 0  2013-04-27 16:43:35   \n",
       "3         7          2            NaN                 0  2013-03-03 10:53:57   \n",
       "4         7          2            0.0                 0  2013-03-27 15:59:13   \n",
       "\n",
       "  my_tags  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "3     NaN  \n",
       "4     NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_score is the label which can be joined together user_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = scores_raw[['username', 'anime_id', 'my_score', 'my_status']]\n",
    "usernames_and_ids = users_raw[['username', 'user_id']]\n",
    "\n",
    "reviews_df = reviews_df.merge(anime_raw[['anime_id', 'title']], left_on='anime_id', right_on='anime_id')\n",
    "reviews_df = reviews_df.merge(usernames_and_ids[['username', 'user_id']], left_on='username', right_on='username')\n",
    "# Remove scores with 0 - it's not a possible value acccording to MyAnimeList\n",
    "reviews_df = reviews_df[reviews_df['my_score'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>my_score</th>\n",
       "      <th>my_status</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>One Piece</td>\n",
       "      <td>2255153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Chobits</td>\n",
       "      <td>2255153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>74</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Gakuen Alice</td>\n",
       "      <td>2255153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>120</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Fruits Basket</td>\n",
       "      <td>2255153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>178</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Ultra Maniac</td>\n",
       "      <td>2255153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   username  anime_id  my_score  my_status          title  user_id\n",
       "0  karthiga        21         9          1      One Piece  2255153\n",
       "1  karthiga        59         7          2        Chobits  2255153\n",
       "2  karthiga        74         7          2   Gakuen Alice  2255153\n",
       "3  karthiga       120         7          2  Fruits Basket  2255153\n",
       "4  karthiga       178         7          2   Ultra Maniac  2255153"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a crude way to check for sparsity of the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_per_anime = reviews_df['anime_id'].value_counts().reset_index().rename(columns={\"count\": \"number_of_users\"})\n",
    "anime_per_user = reviews_df['username'].value_counts().reset_index().rename(columns={\"count\": \"number_of_animes\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There wre a bunch of users where they have only review very few animes, making the user entry very sparse. This will not be good for the collaborative filtering algo. Let's keep it in for more as when we get to more advanced collaborative filtering techniques which uses content-based filtering and hybrid, these \"user who review little\" cases will still be useful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_per_anime[user_per_anime['number_of_users'] < 10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3926, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_per_user[anime_per_user['number_of_animes'] < 10].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only taking users that have made 10 ratings or more is a bit arbitary but it's something to work with now. This can be tweaked if the model is not performing well.\n",
    "I can filter this based on `value_counts()` and then join it back in with inner join on the username and anime_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>my_score</th>\n",
       "      <th>my_status</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "      <th>number_of_animes</th>\n",
       "      <th>number_of_users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>One Piece</td>\n",
       "      <td>2255153</td>\n",
       "      <td>53</td>\n",
       "      <td>31030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Damonashu</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>One Piece</td>\n",
       "      <td>37326</td>\n",
       "      <td>194</td>\n",
       "      <td>31030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bskai</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>One Piece</td>\n",
       "      <td>228342</td>\n",
       "      <td>402</td>\n",
       "      <td>31030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Slimak</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>One Piece</td>\n",
       "      <td>61677</td>\n",
       "      <td>223</td>\n",
       "      <td>31030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kioniel</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>One Piece</td>\n",
       "      <td>144049</td>\n",
       "      <td>296</td>\n",
       "      <td>31030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125480</th>\n",
       "      <td>Benku</td>\n",
       "      <td>35909</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>Guruguru Petit Anime Gekijou</td>\n",
       "      <td>3781121</td>\n",
       "      <td>1029</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125481</th>\n",
       "      <td>Oyakatasama4ever</td>\n",
       "      <td>35909</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>Guruguru Petit Anime Gekijou</td>\n",
       "      <td>39242</td>\n",
       "      <td>648</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125482</th>\n",
       "      <td>Hancock92</td>\n",
       "      <td>35909</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Guruguru Petit Anime Gekijou</td>\n",
       "      <td>5082848</td>\n",
       "      <td>954</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125483</th>\n",
       "      <td>Madison_Brown</td>\n",
       "      <td>35909</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>Guruguru Petit Anime Gekijou</td>\n",
       "      <td>4719207</td>\n",
       "      <td>788</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125484</th>\n",
       "      <td>hugoymh</td>\n",
       "      <td>35909</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>Guruguru Petit Anime Gekijou</td>\n",
       "      <td>887207</td>\n",
       "      <td>382</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19125485 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  username  anime_id  my_score  my_status  \\\n",
       "0                 karthiga        21         9          1   \n",
       "1                Damonashu        21        10          1   \n",
       "2                    bskai        21         8          1   \n",
       "3                   Slimak        21        10          1   \n",
       "4                  kioniel        21         9          4   \n",
       "...                    ...       ...       ...        ...   \n",
       "19125480             Benku     35909         7          6   \n",
       "19125481  Oyakatasama4ever     35909        10          2   \n",
       "19125482         Hancock92     35909         2          2   \n",
       "19125483     Madison_Brown     35909         6          2   \n",
       "19125484           hugoymh     35909         8          2   \n",
       "\n",
       "                                 title  user_id  number_of_animes  \\\n",
       "0                            One Piece  2255153                53   \n",
       "1                            One Piece    37326               194   \n",
       "2                            One Piece   228342               402   \n",
       "3                            One Piece    61677               223   \n",
       "4                            One Piece   144049               296   \n",
       "...                                ...      ...               ...   \n",
       "19125480  Guruguru Petit Anime Gekijou  3781121              1029   \n",
       "19125481  Guruguru Petit Anime Gekijou    39242               648   \n",
       "19125482  Guruguru Petit Anime Gekijou  5082848               954   \n",
       "19125483  Guruguru Petit Anime Gekijou  4719207               788   \n",
       "19125484  Guruguru Petit Anime Gekijou   887207               382   \n",
       "\n",
       "          number_of_users  \n",
       "0                   31030  \n",
       "1                   31030  \n",
       "2                   31030  \n",
       "3                   31030  \n",
       "4                   31030  \n",
       "...                   ...  \n",
       "19125480               11  \n",
       "19125481               11  \n",
       "19125482               11  \n",
       "19125483               11  \n",
       "19125484               11  \n",
       "\n",
       "[19125485 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_per_anime = user_per_anime[user_per_anime['number_of_users'] > 10]\n",
    "anime_per_user = anime_per_user[anime_per_user['number_of_animes'] > 10]\n",
    "\n",
    "reviews_df = pd.merge(reviews_df, anime_per_user, left_on = 'username', right_on = 'username', how = 'inner')\n",
    "reviews_df = pd.merge(reviews_df, user_per_anime, left_on = 'anime_id', right_on = 'anime_id', how = 'inner')\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Drop the columns which we don't need and save to csv so it can be used for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df[['anime_id', 'user_id', 'my_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>my_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>2255153</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>37326</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>228342</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>61677</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>144049</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125480</th>\n",
       "      <td>35909</td>\n",
       "      <td>3781121</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125481</th>\n",
       "      <td>35909</td>\n",
       "      <td>39242</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125482</th>\n",
       "      <td>35909</td>\n",
       "      <td>5082848</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125483</th>\n",
       "      <td>35909</td>\n",
       "      <td>4719207</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125484</th>\n",
       "      <td>35909</td>\n",
       "      <td>887207</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19125485 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          anime_id  user_id  my_score\n",
       "0               21  2255153         9\n",
       "1               21    37326        10\n",
       "2               21   228342         8\n",
       "3               21    61677        10\n",
       "4               21   144049         9\n",
       "...            ...      ...       ...\n",
       "19125480     35909  3781121         7\n",
       "19125481     35909    39242        10\n",
       "19125482     35909  5082848         2\n",
       "19125483     35909  4719207         6\n",
       "19125484     35909   887207         8\n",
       "\n",
       "[19125485 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_csv('processed_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering using Single Value Decomposition\n",
    "\n",
    "Collaborative Filtering is a technique that is based on the premise that users who share similar tastes in the past will likely have similar preferences in the future. One of the methods that I will be covering the implementation of is the Singular Value Decomposition (SVD), a mathematical technique that helps uncover patterns and latent factors within a user-item interaction matrix. By reducing the dimensionality of the data through singular value decomposition, it extracts latent factors that explain user preferences and item characteristics. These latent factors might represent attributes like genres, themes, animation styles, or narrative structures in the context of anime.\n",
    "\n",
    "If people are interested in the mathematical theory behind this, I will write it in another article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = sp.Reader(rating_scale=(1, 10))\n",
    "# The sp.Dataset must accept the columns in a specfic order of user_id, item_id and rating.\n",
    "reviews_df = pd.read_csv('processed_reviews.csv')\n",
    "data = sp.Dataset.load_from_df(reviews_df[['user_id', 'anime_id', 'my_score']], reader)\n",
    "\n",
    "# If you have already saved the processed reviews into a file you can use this line instead.\n",
    "#data = sp.Dataset.load_from_file('processed_reviews.csv', reader=reader)\n",
    "\n",
    "# By default the split is 0.8 to train and 0.2 to test.\n",
    "trainset, testset = sp.model_selection.split.train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.1811  1.1798  1.1797  1.1810  1.1807  1.1805  0.0006  \n",
      "MAE (testset)     0.8708  0.8704  0.8703  0.8708  0.8707  0.8706  0.0002  \n",
      "Fit time          250.25  256.27  258.06  259.64  263.60  257.56  4.39    \n",
      "Test time         114.16  121.74  98.07   115.30  102.41  110.33  8.75    \n"
     ]
    }
   ],
   "source": [
    "algo = sp.SVD()\n",
    "# Run 5-fold cross-validation and print results using RMSE (Root mean square error) and MAE (mean absolute error)\n",
    "results = sp.model_selection.cross_validate(algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import dump\n",
    "# Dump algorithm and reload it.\n",
    "dump.dump('algo.pickle', algo=algo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, loaded_algo = dump.load('algo.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid Out of Memory errors, I'm going to test it with a single user rather than trying to backfill with rating estimates for each user who hasn't rated shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>my_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>61677</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30477</th>\n",
       "      <td>30477</td>\n",
       "      <td>59</td>\n",
       "      <td>61677</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134373</th>\n",
       "      <td>134373</td>\n",
       "      <td>269</td>\n",
       "      <td>61677</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219810</th>\n",
       "      <td>219810</td>\n",
       "      <td>857</td>\n",
       "      <td>61677</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277896</th>\n",
       "      <td>277896</td>\n",
       "      <td>1735</td>\n",
       "      <td>61677</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495287</th>\n",
       "      <td>7495287</td>\n",
       "      <td>11837</td>\n",
       "      <td>61677</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501421</th>\n",
       "      <td>7501421</td>\n",
       "      <td>17389</td>\n",
       "      <td>61677</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7503764</th>\n",
       "      <td>7503764</td>\n",
       "      <td>22777</td>\n",
       "      <td>61677</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7506745</th>\n",
       "      <td>7506745</td>\n",
       "      <td>31904</td>\n",
       "      <td>61677</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512479</th>\n",
       "      <td>7512479</td>\n",
       "      <td>34451</td>\n",
       "      <td>61677</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  anime_id  user_id  my_score\n",
       "3                 3        21    61677        10\n",
       "30477         30477        59    61677         9\n",
       "134373       134373       269    61677         9\n",
       "219810       219810       857    61677         7\n",
       "277896       277896      1735    61677        10\n",
       "...             ...       ...      ...       ...\n",
       "7495287     7495287     11837    61677         7\n",
       "7501421     7501421     17389    61677         9\n",
       "7503764     7503764     22777    61677         7\n",
       "7506745     7506745     31904    61677         7\n",
       "7512479     7512479     34451    61677         7\n",
       "\n",
       "[223 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = 61677\n",
    "reviews_for_user = pd.read_csv('processed_reviews.csv')\n",
    "reviews_for_user = reviews_for_user[reviews_for_user['user_id']== 61677]\n",
    "reviews_for_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "user_to_test = Dataset.load_from_df(reviews_for_user[['user_id', 'anime_id', 'my_score']], Reader(rating_scale=(1,10)))\n",
    "trainset, testset = sp.model_selection.split.train_test_split(user_to_test)\n",
    "predictions = loaded_algo.test(testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid=61677, iid=2476, r_ui=8.0, est=7.4204642943772825, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=2386, r_ui=5.0, est=6.325046801984353, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=355, r_ui=8.0, est=7.810006611181714, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=7593, r_ui=7.0, est=7.360856065907273, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=24439, r_ui=8.0, est=7.595897803514228, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=4744, r_ui=5.0, est=6.964902337648262, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=411, r_ui=8.0, est=7.721308039301372, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=464, r_ui=6.0, est=6.662791103120198, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=6895, r_ui=7.0, est=8.051689671562059, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=4186, r_ui=8.0, est=7.793201152883065, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=11837, r_ui=7.0, est=6.977573467258667, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=12967, r_ui=7.0, est=6.599432385124466, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=894, r_ui=6.0, est=6.5070604716842695, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=238, r_ui=7.0, est=7.662933973961444, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=21, r_ui=10.0, est=8.8775614411466, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=5, r_ui=7.0, est=7.339598005263653, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=8937, r_ui=7.0, est=7.463854944551425, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=356, r_ui=7.0, est=7.849284484541174, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=12549, r_ui=8.0, est=7.320634763055449, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=2472, r_ui=7.0, est=7.54343656596669, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=1944, r_ui=9.0, est=7.999474873704102, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=3655, r_ui=8.0, est=7.177455696413426, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=175, r_ui=7.0, est=7.016000519473011, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=2787, r_ui=7.0, est=7.63419982376801, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=392, r_ui=9.0, est=8.080451046811463, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=7338, r_ui=7.0, est=7.131513258760638, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=2993, r_ui=9.0, est=8.266813179784613, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=4898, r_ui=8.0, est=8.76609764047539, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=3712, r_ui=8.0, est=8.223649939049626, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=34451, r_ui=7.0, est=7.715545536927751, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=9130, r_ui=7.0, est=8.036979642697313, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=1818, r_ui=8.0, est=8.447919857070314, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=161, r_ui=8.0, est=8.013701732359467, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=4884, r_ui=7.0, est=7.437999833010373, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=6707, r_ui=8.0, est=8.27337616513584, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=10213, r_ui=8.0, est=7.966206114646506, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=1735, r_ui=10.0, est=9.724242208574005, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=22777, r_ui=7.0, est=7.228840889907684, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=14467, r_ui=8.0, est=7.391511901997518, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=4151, r_ui=6.0, est=7.304314757444954, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=16498, r_ui=10.0, est=9.313364992402638, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=1691, r_ui=7.0, est=7.817862680053874, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=953, r_ui=6.0, est=6.522609707919314, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=376, r_ui=9.0, est=7.711380849615253, details={'was_impossible': False}),\n",
       " Prediction(uid=61677, iid=133, r_ui=8.0, est=8.244702728993772, details={'was_impossible': False})]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algo `SVD` will contain the model which we can use to predict the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61677 ['Naruto: Shippuuden', 'Shingeki no Kyojin', 'One Piece', 'Kuroshitsuji', 'Claymore', 'Kuroshitsuji II', 'Rosario to Vampire', 'Green Green', 'Zero no Tsukaima: Princesses no Rondo', 'Yuu☆Yuu☆Hakusho']\n"
     ]
    }
   ],
   "source": [
    "top_n = get_top_n(predictions, n=10)\n",
    "anime_raw = pd.read_csv('./data/full/anime_cleaned.csv')\n",
    "\n",
    "\n",
    "def get_title_by_id(anime_id, anime_raw):\n",
    "    title = anime_raw.loc[anime_raw['anime_id'] == anime_id, 'title'].values\n",
    "    if len(title) > 0:\n",
    "        return title[0]\n",
    "    else:\n",
    "        return \"Anime not found\"\n",
    "# For the first 10 users Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [get_title_by_id(iid, anime_raw) for (iid, _) in user_ratings])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering using neural networks\n",
    "Collaborative Filtering can be done by other techniques such as matrix factorization (SVD - Single Value decomposition using the surpise library).\n",
    "Neural networks mathematically will do the same thing with the use of embedding layers - both are effectively matrix dimensionality reduction. We can one hot encode the user id and anime ids and draw out any latent features in the output of the embedding layers.\n",
    "Reason for using neural network over matrix factorization is because we can then extend the model further by joining the output of the embedding layer with further features such as details of the anime or demographic data of the user.\n",
    "\n",
    "For this I will use the suprise https://surpriselib.com/ library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "I'm going to use PyTorch because a lot of people use it - documentation is far and wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saw somewhere that I probably should scale the input space so it is within bounds of 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReviewDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx, :]\n",
    "        print(sample)\n",
    "        label = None\n",
    "        # review_data = self.data[]\n",
    "        return sample['username'], sample['anime_id'], sample['my_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ReviewDataSet('processed_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
